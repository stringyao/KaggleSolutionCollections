# Overview: Binary Classification

### Target: Identify which customers will make a specific transaction in the future.
### Metric: AUC
### Data: Tabluar data + Anonymous features
### Special feature: Extreme low correlations (independent?) among variables

## 1st solution by fl2o: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89003#latest-514568

**Solution overview:**

- We created 200 (one per raw feature) categorical features, let's call them "has one feat", with 5 categories that corresponds (for train data) to:

~~~
This value appears at least another time in data with target==1 and no 0;
  
This value appears at least another time in data with target==0 and no 1;

This value appears at least two more time in data with target==0 & 1;

This value is unique in data;

This value is unique in data + test (only including real test samples);
~~~

- The other 200 (one per raw feature) features are numerical, let's call them "not unique feat", and correspond to the raw feature replacing values that are unique in data + test with the mean of the feature.

- I looked at my LGBM trees (with only 3 leafs that's easy to do) and noticed the trees were using the uniqueness information.

- Augmentation + Psedolabeling: We made a LGBM using the shuffle augmentation (duplicate and shuffle 16 times samples with target == 1, 4 for target ==0) and added pseudo label (2700 highest predicted test points as 1 and 2000 lowest as 0). 

- NN with a particular structure: The idea, like many did, was to process all the features belonging to the same group(raw / has one / not unique) independently and in the same way (i.e using same set of weights). That would create sort of embedding of this feature value. What differentiate us is the next step : We did a weighted average of those 200 embeddings which we then feed to a dense layer for final output. This ensure that every feature is treated in the same way. The weights were generated by another NN. The idea is very similar to what attention networks do. Everything was of course optimized end to end. We added on the fly augmentation (for every batch, shuffle the features values that belong to target == 1 / target == 0). 


## 2nd solution by ONODERA: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/88939#latest-514563

**Solution overview:**

**source code:** 

https://github.com/KazukiOnodera/santander-customer-transaction-prediction 

https://github.com/KazukiOnodera/santander-customer-transaction-prediction/blob/master/py/990_2nd_place_solution_golf.py

- concat train and test, and then invert some of features

- unpivot all vars(so we have 200k x 200 = 4m train samples) then convert prediction(200k x 200) into odds. We used (9 * p / 1 - p)

- count encoding + count round encoding 

## 3rd solution by Nawid: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/88902#latest-514106

**Solution overview:**

**source code:** https://www.kaggle.com/nawidsayed/lightgbm-and-cnn-3rd-place-solution

- I calculate the unique counts of each faeture seaparately. Based on that I also calculate the density by smoothing the counts and also the deviation as counts/density.

- Many public kernels indicated that the features are independent, conditional on the target. For this reason I train seperate trees for each feature and their respective counts. Using a simple average (of the square root) of all tree predictors achieves around 0.9225 / 0.9205 on public/private LB.

- Use CNN to blend the predictors: I choose the architecure in a way which would ensure feature independence up until the last dense layer. In order to minimize overfitting and utilize the similarity of patterns across different var_x I used convolutional layers. The convolutions are performed across different var_x and at any point the filters only have a single var and their respective features in their field of view. Batch normalization is a great regularizer here and very crucial for the success of the model. The model has a total of 2.8K trainable parameters which is sufficiently low to prevent overfitting. I verified this by splitting train data into train / test with use_experimental = True at the top of the kernel and using test AUC as a gauge. The final prediction is the average of the 7 CNNs trained on every fold.





## 4th solution by Evgeny: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/88970#latest-514923

**Solution overview:**

- I put all vars together in one column (200000*200, 1), add counts as second column, name of features as 3rd categorical column and used LightGBM. AUC for this model was not very high - near .53, but product of all predictions grouped by ID returned .9258 locally and .924 on LB. With other aggregates of predictions (min, max, std etc) we got more by logistical regression model.

- The reason why this approach was the best is the data. All vars had no interactions between each other, but GBM found some fake interactions. With long model it became much harder to find inter-vars interactions, boosting mainly used only feature + count pairs.



## 5th solution by KazAnova * Kun Hao: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/88897#latest-514440 & https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/88929#latest-515106

**Solution overview:**

**source code:** 

https://github.com/tnmichael309/Kaggle-Santander-Customer-Transaction-Prediction-5th-Place-Partial-Solution

- NNs perfomed better than lightgbms, but needed an input shape of Input(shape=(200,2)). The first one represents the features and the second the unique count associated with these features. 

- For lightgbm it helped using minmaxscaler of (-4,4) on the original features and then doing Xn** countn where the count is clipped between 1 and 3

- Stacking added around +0.001. NN was significantly better at stacking than any other method.

- NN: where with input shape=(200,), and a following dense layer, say D-1, we are using D-1 to create feature interactions. If the input shape=(200, 1) and a dense layer, say D-2, as we could observe from the model summary (in keras), D-2 is a dense layer shared among those 200 features, i.e., NN is now using D-2 to seek for a common representation across these 200 features, which turns out to be more effective. 

- Now, the 200 additional magic features, how should we combine them? If we use input shape=(400,1), then all the features are assumed to be lack of interactions between each other. But since those magic features are created based on the raw features, interaction between raw-magic feature could be interesting, and turns out to be effective after experiments. So our NN's input shape turns out to be (200, 2), with (200,1) from raw and the other (200,1) from magic. 

- Adding a data loader with on-the-fly augmentation. 





## 7th solution by The Zoo: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89023#latest-514533

**Solution overview:**

**source code:** 

https://www.kaggle.com/dott1718/922-in-3-minutes

https://www.kaggle.com/titericz/giba-single-model-public-0-9245-private-0-9234

- Our models focused on modeling each feature separately and then combining the individual predictions with product or mean logit.

- Pivot LGB: fit one LGB on all features together, but just model it as one feature and have a separate indicator of which feature it is.

- NN: we use 200 Dense layers for each feature separately including large dropouts (0.8) and then just have a final layer lazily combining them.

- NN-Boost: We use abovementioned NN structure, and predict the 200 Dense layers separately for each feature. So we end up with 200 * 512 features. We then fit an LGB separately on each feature and combine predictions.

- Ensemble: We use a hill climber algo for combining. 




