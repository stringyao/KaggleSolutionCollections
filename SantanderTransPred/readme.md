# Overview: Binary Classification

### Target: Identify which customers will make a specific transaction in the future.
### Metric: AUC
### Data: Tabluar data + Anonymous features
### Special feature: Extreme low correlations (independent?) among variables

## 1st solution by fl2o: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89003#latest-514568

**Solution overview:**

- We created 200 (one per raw feature) categorical features, let's call them "has one feat", with 5 categories that corresponds (for train data) to:

~~~
This value appears at least another time in data with target==1 and no 0;
  
This value appears at least another time in data with target==0 and no 1;

This value appears at least two more time in data with target==0 & 1;

This value is unique in data;

This value is unique in data + test (only including real test samples);
~~~

- The other 200 (one per raw feature) features are numerical, let's call them "not unique feat", and correspond to the raw feature replacing values that are unique in data + test with the mean of the feature.

- I looked at my LGBM trees (with only 3 leafs that's easy to do) and noticed the trees were using the uniqueness information.

- Augmentation + Psedolabeling: We made a LGBM using the shuffle augmentation (duplicate and shuffle 16 times samples with target == 1, 4 for target ==0) and added pseudo label (2700 highest predicted test points as 1 and 2000 lowest as 0). 

- NN with a particular structure: The idea, like many did, was to process all the features belonging to the same group(raw / has one / not unique) independently and in the same way (i.e using same set of weights). That would create sort of embedding of this feature value. What differentiate us is the next step : We did a weighted average of those 200 embeddings which we then feed to a dense layer for final output. This ensure that every feature is treated in the same way. The weights were generated by another NN. The idea is very similar to what attention networks do. Everything was of course optimized end to end. We added on the fly augmentation (for every batch, shuffle the features values that belong to target == 1 / target == 0). 


## 2nd solution by ONODERA: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/88939#latest-514563

**Solution overview:**

**source code:** 

https://github.com/KazukiOnodera/santander-customer-transaction-prediction 

https://github.com/KazukiOnodera/santander-customer-transaction-prediction/blob/master/py/990_2nd_place_solution_golf.py

- concat train and test, and then invert some of features

- unpivot all vars(so we have 200k x 200 = 4m train samples) then convert prediction(200k x 200) into odds. We used (9 * p / 1 - p)

- count encoding + count round encoding 

## 7th solution by The Zoo: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89023#latest-514533

**Solution overview:**

**source code:** 

https://www.kaggle.com/dott1718/922-in-3-minutes

https://www.kaggle.com/titericz/giba-single-model-public-0-9245-private-0-9234

- Our models focused on modeling each feature separately and then combining the individual predictions with product or mean logit.

- Pivot LGB: fit one LGB on all features together, but just model it as one feature and have a separate indicator of which feature it is.

- NN: we use 200 Dense layers for each feature separately including large dropouts (0.8) and then just have a final layer lazily combining them.

- NN-Boost: We use abovementioned NN structure, and predict the 200 Dense layers separately for each feature. So we end up with 200 * 512 features. We then fit an LGB separately on each feature and combine predictions.

- Ensemble: We use a hill climber algo for combining. 




