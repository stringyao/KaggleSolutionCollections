# Overview: Binary Classification

### Target: Identify which customers will make a specific transaction in the future.
### Metric: AUC
### Data: Tabluar data + Anonymous features
### Special feature: Extreme low correlations (independent?) among variables

## 1st solution by fl2o: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89003#latest-514568

**Solution overview:**

- We created 200 (one per raw feature) categorical features, let's call them "has one feat", with 5 categories that corresponds (for train data) to:

~~~
This value appears at least another time in data with target==1 and no 0;
  
This value appears at least another time in data with target==0 and no 1;

This value appears at least two more time in data with target==0 & 1;

This value is unique in data;

This value is unique in data + test (only including real test samples);
~~~

- The other 200 (one per raw feature) features are numerical, let's call them "not unique feat", and correspond to the raw feature replacing values that are unique in data + test with the mean of the feature.

- I looked at my LGBM trees (with only 3 leafs that's easy to do) and noticed the trees were using the uniqueness information.

- Augmentation + Psedolabeling: We made a LGBM using the shuffle augmentation (duplicate and shuffle 16 times samples with target == 1, 4 for target ==0) and added pseudo label (2700 highest predicted test points as 1 and 2000 lowest as 0). 

- NN with a particular structure: The idea, like many did, was to process all the features belonging to the same group(raw / has one / not unique) independently and in the same way (i.e using same set of weights). That would create sort of embedding of this feature value. What differentiate us is the next step : We did a weighted average of those 200 embeddings which we then feed to a dense layer for final output. This ensure that every feature is treated in the same way. The weights were generated by another NN. The idea is very similar to what attention networks do. Everything was of course optimized end to end. We added on the fly augmentation (for every batch, shuffle the features values that belong to target == 1 / target == 0). 




